import os
import tensorflow as tf

# ==========================================
# 0. GPU å¼ºåˆ¶é…ç½® (å¿…é¡»æ”¾åœ¨ä»»ä½•å…¶ä»–æ“ä½œä¹‹å‰)
# ==========================================
print(f"å½“å‰ TensorFlow ç‰ˆæœ¬: {tf.__version__}")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… æˆåŠŸå‘ç° GPU: {len(gpus)} ä¸ªï¼Œå·²å¼€å¯æ˜¾å­˜æŒ‰éœ€åˆ†é…ã€‚")
    except RuntimeError as e:
        print(f"âŒ GPU è®¾ç½®é”™è¯¯: {e}")
else:
    print("âš ï¸ æœªå‘ç° GPUï¼å¦‚æœä½ æ˜¯ Windowsï¼Œè¯·ç¡®ä¿å®‰è£…çš„æ˜¯ tensorflow==2.10.0")

# å¯¼å…¥å…¶ä»–åº“ (å¿…é¡»åœ¨ GPU é…ç½®ä¹‹å)
from tensorflow.keras import layers, models, applications, regularizers, mixed_precision
import matplotlib.pyplot as plt
import os.path as path
from datetime import datetime
import numpy as np
import json

# âœ… å¼€å¯æ··åˆç²¾åº¦ (æ–°å¢ä»£ç )
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)
print(f"âš¡ æ··åˆç²¾åº¦å·²å¼€å¯: {policy.compute_dtype}")

# ======================
# 1. å…¨å±€é…ç½®
# ======================
CONFIG = {
    "DATASET_PATH": os.path.join("dataset", "plantVillage", "train"),  # ä½ çš„è®­ç»ƒé›†æ•°æ®é›†è·¯å¾„
    "MODEL_DIR_ROOT": "models",
    "LABEL_DIR_ROOT": "labels",
    "IMG_SIZE": (224, 224),
    "BATCH_SIZE": 64,  # 3050 æ˜¾å­˜è¾ƒå°ï¼Œä¿æŒ 16 æ¯”è¾ƒç¨³
    "EPOCHS": 5,
    "LEARNING_RATE": 1e-3,
    "SEED": 100,
    "VAL_RATE": 0.2,
}


# ======================
# 2. å·¥å…·å‡½æ•°
# ======================
def ensure_dirs_exist():
    for d in [CONFIG["MODEL_DIR_ROOT"], CONFIG["LABEL_DIR_ROOT"]]:
        if not path.exists(d):
            os.makedirs(d)


def load_datasets(data_path, img_size, batch_size, seed, val_rate):
    print("ğŸ”„ Loading datasets from:", data_path)
    # è®­ç»ƒé›†
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path, validation_split=val_rate, subset="training",
        seed=seed, image_size=img_size, batch_size=batch_size,
        label_mode="categorical", shuffle=True
    )
    # éªŒè¯é›†
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path, validation_split=val_rate, subset="validation",
        seed=seed, image_size=img_size, batch_size=batch_size,
        label_mode="categorical", shuffle=True
    )

    class_names = train_ds.class_names
    print(f"ğŸ“Š Detected classes: {class_names}")

    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
    return train_ds, val_ds, class_names


# ======================
# 3. æ¨¡å‹æ„å»º
# ======================
def build_model_graph(num_classes, img_size):
    # æ•°æ®å¢å¼º
    data_augmentation = tf.keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.2),
        layers.RandomZoom(0.2),
    ], name="data_augmentation")

    # é¢„å¤„ç†
    preprocess_input = applications.mobilenet_v2.preprocess_input

    # åŸºç¡€æ¨¡å‹
    base_model = applications.MobileNetV2(
        input_shape=(*img_size, 3),
        include_top=False,
        weights="imagenet"
    )
    base_model.trainable = False

    inputs = tf.keras.Input(shape=(*img_size, 3))
    x = data_augmentation(inputs)
    x = preprocess_input(x)
    x = base_model(x, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation="softmax", dtype="float32")(x)

    model = tf.keras.Model(inputs, outputs)
    return model


# ======================
# 4. ä¿å­˜ä¸è¾…åŠ©
# ======================
def save_for_flask(model, class_names):
    # ä¿å­˜ .h5
    h5_path = path.join(CONFIG["MODEL_DIR_ROOT"], "plant_disease_model.h5")
    model.save(h5_path)
    print(f"â˜ï¸ [Flask] Model saved: {h5_path}")

    # ä¿å­˜ JSON
    indices_dict = {str(i): name for i, name in enumerate(class_names)}
    json_path = path.join(CONFIG["MODEL_DIR_ROOT"], "class_indices.json")
    with open(json_path, 'w') as f:
        json.dump(indices_dict, f, indent=4)


def plot_history(history, save_path):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs_range = range(len(acc))

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Accuracy')
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Loss')
    plt.savefig(save_path)


# ======================
# 5. ä¸»ç¨‹åº
# ======================
if __name__ == "__main__":
    ensure_dirs_exist()

    # 1. åŠ è½½æ•°æ®
    if not os.path.exists(CONFIG["DATASET_PATH"]):
        print(f"âŒ Error: æ‰¾ä¸åˆ°æ•°æ®é›†: {CONFIG['DATASET_PATH']}")
        exit()

    train_ds, val_ds, class_names = load_datasets(
        CONFIG["DATASET_PATH"], CONFIG["IMG_SIZE"], CONFIG["BATCH_SIZE"],
        CONFIG["SEED"], CONFIG["VAL_RATE"]
    )

    # 2. æ„å»ºä¸ç¼–è¯‘
    print("\nğŸ”¨ Building Model...")
    model = build_model_graph(len(class_names), CONFIG["IMG_SIZE"])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG["LEARNING_RATE"]),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )

    # 3. è®­ç»ƒ
    callbacks = [
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),
        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
    ]

    print(f"\nğŸš€ Starting Training...")
    history = model.fit(train_ds, epochs=CONFIG["EPOCHS"], validation_data=val_ds, callbacks=callbacks)

    # 4. ä¿å­˜
    save_for_flask(model, class_names)
    plot_history(history, path.join(CONFIG["MODEL_DIR_ROOT"], "training_curve.png"))

    print(f"\nâœ… Done! Max Val Accuracy: {max(history.history['val_accuracy']):.2%}")
